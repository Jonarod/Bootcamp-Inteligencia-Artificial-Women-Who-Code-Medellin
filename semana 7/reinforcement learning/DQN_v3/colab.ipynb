{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/billpottle/atari-dqn/blob/master/Atari_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9z2s3Tgxvq8u"
      },
      "source": [
        "A basic tensorflow 2.0 implementation of some of the ideas in DeepMind's papers, \"[Human Level Control Through Deep Reinforcement Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)\" and [Playing Atari with Deep Reinforcement Learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learningusing) Deep Q Learning to play Atari games. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGE8DO-xvsmM"
      },
      "source": [
        "**References** - While the papers themselves are of course the most important references, I owe a lot to various reference implementations, notably Phil Tabor's [youtube video](https://www.youtube.com/watch?v=a5XbO5Qgy5w) and [Udemy Course](https://www.udemy.com/course/deep-q-learning-from-paper-to-code/). Although it is in tensorflow 1, I highly recommend this [extremely thorough explanation](https://colab.research.google.com/github/fg91/Deep-Q-Learning/blob/master/DQN.ipynb) with code that trains very quickly. I also recommend this [great medium article ](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sC6tWuvNvwqz"
      },
      "source": [
        "You can follow along and run the code by clicking on the 'play' icon to the left of each cell, or click 'Run all' from the runtime menu. \n",
        "\n",
        "The first thing we will do is import the various libraries that we will need. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "neG4f1IHUutt",
        "outputId": "e50e96e2-ff3f-42fc-8b1e-5d60cbc6f093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.22.2\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "%pip install -U autorom\n",
        "\n",
        "import tensorflow as tf\n",
        "# from tensorflow.python.framework.ops import disable_eager_execution\n",
        "#disable_eager_execution()\n",
        "import os\n",
        "import imageio\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Ye5-HzxwA-M"
      },
      "source": [
        "Next we will set our global variables. You can play around with different values and try to get a new high score. The network should be able to [train any available Atari game](https://gym.openai.com/envs/#atari). We usually want the deterministic-v4 environments. Make sure whichever game you want is the only one not commented out. Although this should be able to train any game, the model is game specific. A model trained on one game can't play other games. \n",
        "\n",
        "Variables in all caps are constants that don't change during the run. You can change them and see how it changes the results and training time. Don't change anything in lower case, as the program will change them during the run. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RZTchNmLc_Ma"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'PongNoFrameskip-v4'\t\t\t\t# Valid ATARI Game. For example: PongNoFrameskip-v4, BreakoutDeterministic-v4, MsPacmanNoFrameskip-v4, KungFuMasterNoFrameskip-v4\n",
        "\n",
        "PATH = \"output/\"  \t\t\t\t\t\t\t# Where to save the model and gif files\n",
        "os.makedirs(PATH, exist_ok=True)\n",
        "LOAD_CHECKPOINT \t\t= False \t\t\t# In case you want to save a model file and then train it more later. \n",
        "LOAD_FILE \t\t\t\t= 'atari_model.h5' \t# Filename \n",
        "\n",
        "EVALUATION_FREQUENCY \t= 60\t\t\t\t# Evaluate model once every ... games\n",
        "SAVE_FREQUENCY \t\t\t= 100\t\t\t\t# Save model once every ... games\n",
        "MODEL_COPY_FREQUENCY \t= 3\t\t\t\t\t# Copy model once every ... games\n",
        "\n",
        "MAX_MEMORY_LENGTH\t\t= 15000 \t\t\t# Size of the memory in frames. Reduce this if you get out of RAM errors\n",
        "BATCH_SIZE\t\t\t \t= 32 \t\t\t\t# How many memory samples to train on\n",
        "\n",
        "NUM_GAMES \t\t\t\t= 2000 \t\t\t\t# Reduce this if it takes too long to train, increase it if you need more time to learn. \n",
        "\n",
        "FRAME_HEIGHT \t\t\t= 80 \t\t\t\t# Smaller frames reduce training time but could lose info\n",
        "FRAME_WIDTH \t\t\t= 80 \t\t\t\t# Especially for smaller items like knives or dots \n",
        "\n",
        "INITIAL_EXP \t\t\t= 15000 \t\t\t# Number of random actions to start out with\n",
        "EPS_MIN \t\t\t\t= 0.05 \t\t\t\t# Mimimum chance to take a random action\n",
        "EPS_DEC \t\t\t\t= 0.00001 \t\t\t# Amount to decrease random chance every frame\n",
        "\n",
        "ALPHA \t\t\t\t\t= 0.0001 \t\t\t# Learning rate -  how fast network parameters are updated. Increasing this will let you learn faster, but you may 'overshoot'\n",
        "GAMMA \t\t\t\t\t= 0.99 \t\t\t\t# Future reward discount\n",
        "\n",
        "eps \t\t\t= 1.0 \t\t# Chance to take a random action\n",
        "n_frames \t\t= 0 \t\t# The cumulative number of frames the agent has seen\n",
        "evaluation \t\t= False \t# Don't change here, it will\n",
        "frames_for_gif \t= []\t\t# Array to store gif frames\n",
        "\n",
        "\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# NOT NECESSARY ON M1 MACS: GPU is automatically detected\n",
        "# We define this function so recent versions of keras and tf will work together\n",
        "# def _get_available_gpus():\n",
        "#   #global _LOCAL_DEVICES\n",
        "#   if tfback._LOCAL_DEVICES is None:\n",
        "#     devices = tf.config.list_logical_devices()\n",
        "#     tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "#   return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "# tfback._get_available_gpus = _get_available_gpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i00XUUe1xjJG"
      },
      "source": [
        "This type of training would take a LONG time on even the fastest CPU. GPUs developed for video games are good at doing many simple things at once, and thus they are indispensible for training neural networks. Under Runtime, go to 'Change runtime type' and make sure you are using a GPU. The following code will tell you which GPU you have. \n",
        "\n",
        "Google will assign you one and let you use it for up to 12 hours. For $10/mo, [Colab Pro](https://colab.research.google.com/signup), will give you more powerful GPUs, more memory, and 24 hour runtimes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "colab_type": "code",
        "id": "2KCSzqXEmKx2",
        "outputId": "35ea513e-4c1b-48c8-bcf8-1bf43840ad12"
      },
      "outputs": [],
      "source": [
        "# Only for Google Colab\n",
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "#   print('and then re-execute this cell.')\n",
        "# else:\n",
        "#   print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YMKKy9pyx5Ol"
      },
      "source": [
        "**Random Agent** - The following code has a Random Agent play the game and then displays the last frame of the game. This can be useful if you are thinking about cropping the image to remove things like the score, level, or health bars.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "colab_type": "code",
        "id": "KKL_asTjdPcI",
        "outputId": "2347aefb-d7fd-4f76-f884-8efce50cbb8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To play this game, on each frame the player chooses one of 6 possible actions:\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe1aaa54080>"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create the environment\n",
        "env = gym.make(ENV_NAME).env\n",
        "# Reset it, returns the starting frame\n",
        "frame = env.reset()\n",
        "print(\"To play this game, on each frame the player chooses one of {} possible actions:\".format(env.action_space.n))\n",
        "print(\"{}\".format(env.unwrapped.get_action_meanings()))\n",
        "done = False\n",
        "while not done:\n",
        "  # Perform a random action, returns the new frame, reward and whether the game is over\n",
        "  frame, reward, done, info = env.step(env.action_space.sample())\n",
        "env.close() \n",
        "plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t0jhVb7Syr11"
      },
      "source": [
        "Going from 3 channels (RGB) to 1 (BW) gives us a 3x reduction in size. Cropping and scaling from 210 X 160 to 80 X 80 gives us a further 5.25 reduction in size. So we can train something in 1 day that would take 15.75 days on a full frame. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "colab_type": "code",
        "id": "Fpu_B0fJdT7B",
        "outputId": "b485279f-8463-4c23-b2a7-7bf1db54a62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Frame Size: (210, 160, 3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe1a00d0f60>"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Original Frame Size:\", frame.shape)\n",
        "processed = tf.image.rgb_to_grayscale(frame)\n",
        "processed = tf.image.resize(processed,[FRAME_HEIGHT, FRAME_WIDTH],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "processed = np.asarray(processed).reshape(FRAME_HEIGHT,FRAME_WIDTH)\n",
        "processed = np.asarray(processed)\n",
        "plt.gray()\n",
        "plt.imshow(processed) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NvVGbFvS0W1l"
      },
      "source": [
        "**Exploration vs Exploitation** - Say there are 100 restaurants in your town. You've been to the 10 nearby, and really like 2 or 3. When you finally get that date night, do you hit up one that you know will be good, or do you venture out to try something new? If you never try something new, there is a 90% chance that you will miss the best restaurant out there. \n",
        "\n",
        "Our agent has a similar issue. Should it try the move it thinks is best, or try a random move with the chance to come up with a better strategy? \n",
        "\n",
        "The following code will tell it which to do - When we start out, we are going to do only random actions. After some amount of time, we are going to slowly decrease the amount of random actions we take. We will reach a plateau, though, because we always want to be taking at least a few random chances. \n",
        "\n",
        "Of course, during evaulation mode, we want the agent to only do what it thinks is best, so we will never take random actions when we are testing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pDr48tOGVnfo"
      },
      "outputs": [],
      "source": [
        "def ExplorerExploiter():\n",
        "\n",
        "  # Never take a random action during evaluation games\n",
        "  if evaluation: \n",
        "    return 'Exploit'\n",
        "\n",
        "  # Take only random actions during initial exploration phase\n",
        "  if n_frames < INITIAL_EXP:\n",
        "    return 'Explore'\n",
        "\n",
        "  # Explore with chance eps\n",
        "  if np.random.random() < eps:\n",
        "    return 'Explore'\n",
        "\n",
        "  # Otherwise exploit\n",
        "  return 'Exploit'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d3e5BOLa0fKe"
      },
      "source": [
        "The main idea behind deep Q learning is to create a neural network that takes the pixels of the game board in as input and outputs the best action. The final output layer has one node for each possible action - the value of the node for the input pixels is what value we expect to get from taking that action (ie, fire, jump, turn left) in the given state (input of all pixels on the screen). **When training, we continually adjust the network so that the weights are close to the actual value received for taking a move in a given state.** Then, for new states, we pick the move that network says will give us the highest reward + future reward. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GLWchMAE0jGG"
      },
      "source": [
        "We could train on each frame or game, but it's much more effecient to create a Memory that has a large number of previous frames and their results. When training, we simply sample from that memory.\n",
        "\n",
        "Each 'memory' consists of the state (pixels), what action we took, what reward we got, what the next state was, and whether or not the game ended. If you are running the notebook and crash due to a memory error, you should still be able to get good results with a smaller memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_-3-c8wOMYM4"
      },
      "outputs": [],
      "source": [
        "class ExperienceMemory(object):\n",
        "  def __init__(self, max_memory_length):\n",
        "      self.state_memory = deque(maxlen=max_memory_length)\n",
        "      self.action_memory = deque(maxlen=max_memory_length)\n",
        "      self.reward_memory = deque(maxlen=max_memory_length)\n",
        "      self.next_state_memory = deque(maxlen=max_memory_length)\n",
        "      self.done_memory = deque(maxlen=max_memory_length)\n",
        "      \n",
        "  def get_length(self):\n",
        "      return len(self.state_memory)\n",
        "\n",
        "  # Store a new memory    \n",
        "  def store_transition(self, state, action, reward, next_state, done):\n",
        "      self.state_memory.append(state)\n",
        "      self.action_memory.append(action)\n",
        "      self.reward_memory.append(reward)\n",
        "      self.next_state_memory.append(next_state)\n",
        "      self.done_memory.append(done)\n",
        "\n",
        "  # Get out batch_size samples from the memory\n",
        "  def sample_buffer(self, batch_size):\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_states = []\n",
        "    dones = []\n",
        "  \n",
        "    for i in range(batch_size):\n",
        "      sample_id  = np.random.randint(len(self.state_memory))\n",
        "      states.append(self.state_memory[sample_id])\n",
        "      actions.append(self.action_memory[sample_id])\n",
        "      rewards.append(self.reward_memory[sample_id])\n",
        "      next_states.append(self.next_state_memory[sample_id])\n",
        "      dones.append(self.done_memory[sample_id])\n",
        "\n",
        "    return np.asarray(states), np.asarray(actions), np.asarray(rewards), np.asarray(next_states), np.asarray(dones)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BAW2htlo1K5P"
      },
      "source": [
        "**Atari Wrappers** - The Atari environment outputs 'raw' frames, so we need to apply these wrappers to format the frames to be what our agent should see. \n",
        "\n",
        "The Skip wrapper helps us pool the result from 4 frames, since sometimes the atari environments can skil\n",
        "\n",
        "The PreProcessFrame wrapper converts the frame to grayscale, while the move image channels wrapper gets the channels into a format our DQN can use. \n",
        "\n",
        "The Normalize Frame wrapper puts all pixel values to between 0 and 1, which helps with the math. We could also clip the reward - this helps on some games as well. \n",
        "\n",
        "The FrameStacker stacks the last 4 frames together - this is important because our agent needs to understand motion. With only one image, how could it tell which direction a ball was going? With 2 it could tell direction and speed, but not acceleration. Most implementations use 4 frames, because it is an even number and it gives some better historical information about the movement of objects. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Lsd1aLx_dye5"
      },
      "outputs": [],
      "source": [
        "class SkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(SkipEnv, self).__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            t_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "         # When we are in evaluation mode, we need to send the deepest wrapped \n",
        "        # (original) observation back out so that we can see how the agent plays\n",
        "        if evaluation == True:\n",
        "          frames_for_gif.append(np.asarray(obs))\n",
        "        return obs, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer = []\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class PreProcessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(PreProcessFrame, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "                                                shape=(FRAME_HEIGHT,FRAME_WIDTH,1), dtype=np.uint8)\n",
        "    def observation(self, obs):\n",
        "        \n",
        "        new_frame = np.reshape(obs, obs.shape).astype(np.float32)\n",
        "        # convert to grayscale\n",
        "        new_frame = tf.image.rgb_to_grayscale(new_frame)\n",
        "        # scale to frame height and width\n",
        "        new_frame = tf.image.resize(new_frame,[FRAME_HEIGHT, FRAME_WIDTH],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        # convert to numpy array\n",
        "        new_frame = np.asarray(new_frame)\n",
        "        return new_frame.astype(np.float32)\n",
        "\n",
        "class MoveImgChannel(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(MoveImgChannel, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "                            shape=(self.observation_space.shape[-1],\n",
        "                                   self.observation_space.shape[0],\n",
        "                                   self.observation_space.shape[1]),\n",
        "                            dtype=np.float32)\n",
        "  \n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class NormalizeFrame(gym.ObservationWrapper):\n",
        "    # The match is easier if everything is normalized to be betwee 0 and 1.\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "class FrameStacker(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(FrameStacker, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                             env.observation_space.low.repeat(n_steps, axis=0),\n",
        "                             env.observation_space.high.repeat(n_steps, axis=0),\n",
        "                             dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "# Apply all the wrappers on top of each other.\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = SkipEnv(env)\n",
        "    env = PreProcessFrame(env)\n",
        "    env = MoveImgChannel(env)\n",
        "    env = FrameStacker(env, 4)\n",
        "    return NormalizeFrame(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "27s7nZNL0Hs_"
      },
      "source": [
        "Here is the code to generate and save the animated gifs. \n",
        "If using colab, you can click on files to the left and then output and download the gifs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GB6LipnZh60C"
      },
      "outputs": [],
      "source": [
        "def generate_gif(frame_number, frames_for_gif, reward):\n",
        "\n",
        "    for idx, frame_idx in enumerate(frames_for_gif):\n",
        "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3),\n",
        "                                     preserve_range=True, order=0).astype(np.uint8)\n",
        "\n",
        "    imageio.mimsave(f'{PATH}{\"{0}_game_{1}_reward_{2}.gif\".format(ENV_NAME, frame_number, reward)}',\n",
        "                    frames_for_gif, duration=1/30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DTFNcPQV2VOF"
      },
      "source": [
        "**Agent** - Our Agent class is the key player. The agent has 2 neural networks, the Q network used for predicting the best action to take in each state, and the target network, used for predicting the value of that action. When there is only one network training is difficult because it's chasing after itself. The q network's values are periodically copied over to the target network. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qfl90rpgdtUK"
      },
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, alpha, n_actions, input_dims):\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.gamma = GAMMA\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.memory= ExperienceMemory(MAX_MEMORY_LENGTH)\n",
        "        self.q_network = self.build_dqn(alpha, n_actions, input_dims)\n",
        "        self.target_network = self.build_dqn(alpha, n_actions, input_dims)\n",
        "\n",
        "    # This is the same architecture used by deep mind\n",
        "    def build_dqn(self, lr, n_actions, input_dims):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(filters=32, kernel_size=8, strides=4, activation='relu',\n",
        "                        input_shape=(*input_dims,), data_format='channels_first'))\n",
        "        model.add(Conv2D(filters=64, kernel_size=4, strides=2, activation='relu',\n",
        "                        data_format='channels_first'))\n",
        "        model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu',\n",
        "                        data_format='channels_first'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(n_actions))\n",
        "\n",
        "        model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
        "        model.summary() # Print network details\n",
        "        return model\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # The agent will make use of our ExplorerExploiter to choose either\n",
        "    # Random actions or the action that gives the highest Q value\n",
        "    def choose_action(self, observation):\n",
        "        \n",
        "        if ExplorerExploiter() == 'Explore': \n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            state = np.array([observation], copy=False, dtype=np.float32)\n",
        "            actions = self.q_network.predict(state)\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def learn(self):\n",
        "      # First of all, make sure we have enough memories to train on.\n",
        "        if self.memory.get_length() > self.batch_size:\n",
        "            # Get a batch of memories. Each is an array of 32 memories\n",
        "            state, action, reward, new_state, done = \\\n",
        "                                    self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "            #Predict both the values we thought we could get. Use the q\n",
        "            # network for the state and the target network for the next state\n",
        "\n",
        "            q_eval = self.q_network.predict(state)\n",
        "            q_next = self.target_network.predict(new_state)\n",
        "\n",
        "            q_target = q_eval[:]\n",
        "\n",
        "            indices = np.arange(self.batch_size)\n",
        "            # Dones is 0 or 1, so this acts as a mask so that when the episode\n",
        "            # is done, we will only take the reward\n",
        "            # When it is not done, we will take the best value reward of the\n",
        "            # next state times the future discount\n",
        "\n",
        "            q_target[indices, action] = reward + \\\n",
        "                                    self.gamma*np.max(q_next, axis=1)*(1 - done)\n",
        "            # finally, train the network to backpropogate the loss\n",
        "            self.q_network.train_on_batch(state, q_target)\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_network.save('Atari Model {}.h5'.format(ENV_NAME))\n",
        "        print('... saving models ...')\n",
        "    \n",
        "    # Restore the model and copy parameters to target network\n",
        "    def load_models(self):\n",
        "        self.q_network = load_model(SAVE_FILE)\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "        print('... loading models ...')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ShzsHLlN5KFo"
      },
      "source": [
        "**Main Loop** - There are two nested loops here - one for each frame of each game, and then the outer loop runs for the specified number of games. \n",
        "\n",
        "Finally, plot out the scores over time. Most agents follow a roughly logrithmic pattern, where they don't learn any more after a certain time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "n_otnehxdcPf",
        "outputId": "502f9a97-c543-4c74-d8c1-e7dd164dbc7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
            "[Powered by Stella]\n",
            "2022-02-23 06:19:11.200001: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-02-23 06:19:11.200113: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "/Users/home/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 19, 19)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 64, 8, 8)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 6, 6)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1180160   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,261,222\n",
            "Trainable params: 1,261,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 32, 19, 19)        8224      \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 64, 8, 8)          32832     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 64, 6, 6)          36928     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               1180160   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,261,222\n",
            "Trainable params: 1,261,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-02-23 06:19:11.586981: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "2022-02-23 06:19:11.621052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-02-23 06:19:12.220377: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-02-23 06:19:12.398948: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "game:  0 score:  -20.0  average score -20.000 epsilon 1.00 steps 987\n",
            "Go you! - last 50 games avg score -20.00 better than best 50 games avg -inf. \n",
            "game:  1 score:  -21.0  average score -20.500 epsilon 1.00 steps 1872\n",
            "game:  2 score:  -20.0  average score -20.333 epsilon 1.00 steps 2859\n",
            "game:  3 score:  -18.0  average score -19.750 epsilon 1.00 steps 3964\n",
            "Go you! - last 50 games avg score -19.75 better than best 50 games avg -20.00. \n",
            "game:  4 score:  -21.0  average score -20.000 epsilon 1.00 steps 4836\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb Cell 26'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000025?line=48'>49</a>\u001b[0m agent\u001b[39m.\u001b[39mstore_transition(observation, action,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000025?line=49'>50</a>\u001b[0m                       reward, observation_, \u001b[39mint\u001b[39m(done))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000025?line=50'>51</a>\u001b[0m \u001b[39m# Train on one batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000025?line=51'>52</a>\u001b[0m agent\u001b[39m.\u001b[39;49mlearn() \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000025?line=52'>53</a>\u001b[0m \u001b[39m# Increment observation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000025?line=53'>54</a>\u001b[0m observation \u001b[39m=\u001b[39m observation_\n",
            "\u001b[1;32m/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb Cell 24'\u001b[0m in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=46'>47</a>\u001b[0m state, action, reward, new_state, done \u001b[39m=\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=47'>48</a>\u001b[0m                         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msample_buffer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=49'>50</a>\u001b[0m \u001b[39m#Predict both the values we thought we could get. Use the q\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=50'>51</a>\u001b[0m \u001b[39m# network for the state and the target network for the next state\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=52'>53</a>\u001b[0m q_eval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_network\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=53'>54</a>\u001b[0m q_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_network\u001b[39m.\u001b[39mpredict(new_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/home/Documents/machine_learning/atari-pong/DQN_v3/main.ipynb#ch0000023?line=55'>56</a>\u001b[0m q_target \u001b[39m=\u001b[39m q_eval[:]\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py:1758\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1749'>1750</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1750'>1751</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1751'>1752</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1752'>1753</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1753'>1754</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1754'>1755</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1755'>1756</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1757'>1758</a>\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1758'>1759</a>\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1759'>1760</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1760'>1761</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1761'>1762</a>\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1762'>1763</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1763'>1764</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1764'>1765</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1765'>1766</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1766'>1767</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1767'>1768</a>\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1769'>1770</a>\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/training.py?line=1770'>1771</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py:1403\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1400'>1401</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1401'>1402</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1402'>1403</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py:1153\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1149'>1150</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1151'>1152</a>\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1152'>1153</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1153'>1154</a>\u001b[0m     x,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1154'>1155</a>\u001b[0m     y,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1155'>1156</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1156'>1157</a>\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1157'>1158</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1158'>1159</a>\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1159'>1160</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1160'>1161</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1161'>1162</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1162'>1163</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1163'>1164</a>\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1164'>1165</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1166'>1167</a>\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=1168'>1169</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py:291\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=284'>285</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m indices\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=286'>287</a>\u001b[0m \u001b[39m# We prefetch a single element. Computing large permutations can take quite\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=287'>288</a>\u001b[0m \u001b[39m# a while so we don't want to wait for prefetching over an epoch boundary to\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=288'>289</a>\u001b[0m \u001b[39m# trigger the next permutation. On the other hand, too many simultaneous\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=289'>290</a>\u001b[0m \u001b[39m# shuffles can contend on a hardware level and degrade all performance.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=290'>291</a>\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mmap(permutation)\u001b[39m.\u001b[39mprefetch(\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=292'>293</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=293'>294</a>\u001b[0m   \u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=294'>295</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=295'>296</a>\u001b[0m \u001b[39m  This step can be accomplished in several ways. The most natural is to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=306'>307</a>\u001b[0m \u001b[39m    A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=307'>308</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2004\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2000'>2001</a>\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2001'>2002</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2002'>2003</a>\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2003'>2004</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39;49m, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2004'>2005</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2005'>2006</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2006'>2007</a>\u001b[0m       \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2007'>2008</a>\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2010'>2011</a>\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=2011'>2012</a>\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:5455\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5452'>5453</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5453'>5454</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality \u001b[39m=\u001b[39m preserve_cardinality\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5454'>5455</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m StructuredFunctionWrapper(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5455'>5456</a>\u001b[0m     map_func,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5456'>5457</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5457'>5458</a>\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5458'>5459</a>\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5459'>5460</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata \u001b[39m=\u001b[39m dataset_metadata_pb2\u001b[39m.\u001b[39mMetadata()\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=5460'>5461</a>\u001b[0m \u001b[39mif\u001b[39;00m name:\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4533\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4525'>4526</a>\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4526'>4527</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4527'>4528</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4528'>4529</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4529'>4530</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4530'>4531</a>\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4532'>4533</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4533'>4534</a>\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4534'>4535</a>\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3244\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3234'>3235</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3235'>3236</a>\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3236'>3237</a>\u001b[0m \n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3237'>3238</a>\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3241'>3242</a>\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3242'>3243</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3243'>3244</a>\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3244'>3245</a>\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3245'>3246</a>\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3246'>3247</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3210\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3207'>3208</a>\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3208'>3209</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3209'>3210</a>\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3210'>3211</a>\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3211'>3212</a>\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3212'>3213</a>\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3557\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3552'>3553</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3553'>3554</a>\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3555'>3556</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3556'>3557</a>\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3557'>3558</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3559'>3560</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3392\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3386'>3387</a>\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3387'>3388</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3388'>3389</a>\u001b[0m ]\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3389'>3390</a>\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3390'>3391</a>\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3391'>3392</a>\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3392'>3393</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3393'>3394</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3394'>3395</a>\u001b[0m         args,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3395'>3396</a>\u001b[0m         kwargs,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3396'>3397</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3397'>3398</a>\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3398'>3399</a>\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3399'>3400</a>\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3400'>3401</a>\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3401'>3402</a>\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3402'>3403</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3403'>3404</a>\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3404'>3405</a>\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3405'>3406</a>\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3406'>3407</a>\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3407'>3408</a>\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3408'>3409</a>\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=3409'>3410</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1142'>1143</a>\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mmap_structure(convert, func_outputs,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m                                   expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1149'>1150</a>\u001b[0m check_mutation(func_args_before, func_args, original_func)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1150'>1151</a>\u001b[0m check_mutation(func_kwargs_before, func_kwargs, original_func)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=864'>865</a>\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=865'>866</a>\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=867'>868</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=868'>869</a>\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=869'>870</a>\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=864'>865</a>\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=865'>866</a>\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=867'>868</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=868'>869</a>\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/nest.py?line=869'>870</a>\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1106\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.convert\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1099'>1100</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1100'>1101</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo be compatible with tf.eager.defun, Python functions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1101'>1102</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmust return zero or more Tensors; in compilation of \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1102'>1103</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn value of type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, which is not a Tensor.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1103'>1104</a>\u001b[0m         (\u001b[39mstr\u001b[39m(python_func), \u001b[39mtype\u001b[39m(x)))\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1104'>1105</a>\u001b[0m \u001b[39mif\u001b[39;00m add_control_dependencies:\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1105'>1106</a>\u001b[0m   x \u001b[39m=\u001b[39m deps_ctx\u001b[39m.\u001b[39;49mmark_as_return(x)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1106'>1107</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py:232\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.mark_as_return\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=226'>227</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor_array_ops\u001b[39m.\u001b[39mbuild_ta_with_new_flow(tensor, flow)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=227'>228</a>\u001b[0m \u001b[39m# We want to make the return values depend on the stateful operations, but\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=228'>229</a>\u001b[0m \u001b[39m# we don't want to introduce a cycle, so we make the return value the result\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=229'>230</a>\u001b[0m \u001b[39m# of a new identity operation that the stateful operations definitely don't\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=230'>231</a>\u001b[0m \u001b[39m# depend on.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=231'>232</a>\u001b[0m tensor \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49midentity(tensor)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=232'>233</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returned_tensors\u001b[39m.\u001b[39madd(tensor)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py?line=233'>234</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1093'>1094</a>\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1094'>1095</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1095'>1096</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1096'>1097</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1097'>1098</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1098'>1099</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1099'>1100</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:290\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=285'>286</a>\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=286'>287</a>\u001b[0m   \u001b[39m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=287'>288</a>\u001b[0m   \u001b[39m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=288'>289</a>\u001b[0m   \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39minput\u001b[39m)\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=289'>290</a>\u001b[0m ret \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49midentity(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=290'>291</a>\u001b[0m \u001b[39m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=291'>292</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_handle_data\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:4077\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py?line=4074'>4075</a>\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py?line=4075'>4076</a>\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py?line=4076'>4077</a>\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py?line=4077'>4078</a>\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mIdentity\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py?line=4078'>4079</a>\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py?line=4079'>4080</a>\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:744\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=738'>739</a>\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=739'>740</a>\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=740'>741</a>\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=741'>742</a>\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=742'>743</a>\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=743'>744</a>\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=744'>745</a>\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=745'>746</a>\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=747'>748</a>\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=748'>749</a>\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=749'>750</a>\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=750'>751</a>\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py?line=751'>752</a>\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:689\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=686'>687</a>\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=687'>688</a>\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=688'>689</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=689'>690</a>\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=690'>691</a>\u001b[0m     compute_device)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3690\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=3686'>3687</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=3687'>3688</a>\u001b[0m   name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_name(name)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=3689'>3690</a>\u001b[0m node_def \u001b[39m=\u001b[39m _NodeDef(op_type, name, attrs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=3691'>3692</a>\u001b[0m input_ops \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(t\u001b[39m.\u001b[39mop \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=3692'>3693</a>\u001b[0m control_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_control_dependencies_for_inputs(input_ops)\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1872\u001b[0m, in \u001b[0;36m_NodeDef\u001b[0;34m(op_type, name, attrs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1858'>1859</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_NodeDef\u001b[39m(op_type, name, attrs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1859'>1860</a>\u001b[0m   \u001b[39m\"\"\"Create a NodeDef proto.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1860'>1861</a>\u001b[0m \n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1861'>1862</a>\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1869'>1870</a>\u001b[0m \u001b[39m    A node_def_pb2.NodeDef protocol buffer.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1870'>1871</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1871'>1872</a>\u001b[0m   node_def \u001b[39m=\u001b[39m node_def_pb2\u001b[39m.\u001b[39mNodeDef(op\u001b[39m=\u001b[39mcompat\u001b[39m.\u001b[39;49mas_bytes(op_type),\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1872'>1873</a>\u001b[0m                                   name\u001b[39m=\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(name))\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1873'>1874</a>\u001b[0m   \u001b[39mif\u001b[39;00m attrs:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1874'>1875</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(attrs):\n",
            "File \u001b[0;32m~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/compat.py:81\u001b[0m, in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/compat.py?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(bytes_or_text, \u001b[39mbytearray\u001b[39m):\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/compat.py?line=79'>80</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mbytes\u001b[39m(bytes_or_text)\n\u001b[0;32m---> <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/compat.py?line=80'>81</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(bytes_or_text, _six\u001b[39m.\u001b[39mtext_type):\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/compat.py?line=81'>82</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m bytes_or_text\u001b[39m.\u001b[39mencode(encoding)\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/base-ai/lib/python3.8/site-packages/tensorflow/python/util/compat.py?line=82'>83</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(bytes_or_text, \u001b[39mbytes\u001b[39m):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Make our environment\n",
        "env = make_env(ENV_NAME)\n",
        "\n",
        "# Make our agent\n",
        "agent = Agent(alpha=ALPHA, input_dims=(4,FRAME_HEIGHT,FRAME_WIDTH), n_actions=env.action_space.n)   \n",
        "\n",
        "# Set the worst possible best score\n",
        "best_score = - math.inf \n",
        "\n",
        "if LOAD_CHECKPOINT:\n",
        "    agent.load_models()\n",
        "\n",
        "scores  = []\n",
        " \n",
        "# Main training loop \n",
        "for i in range(NUM_GAMES):\n",
        "\n",
        "    # Check if it's time to play an evaluation game, save models, \n",
        "    # or copy network parameters   \n",
        "    if i != 0 and i % EVALUATION_FREQUENCY == 0:\n",
        "      evaluation = True\n",
        "    else: \n",
        "      evaluation = False\n",
        "\n",
        "    if i % MODEL_COPY_FREQUENCY == 0:\n",
        "      agent.target_network.set_weights(agent.q_network.get_weights())\n",
        "       \n",
        "    # Reset parameters for each game   \n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    score = 0\n",
        "\n",
        "    # Loop that runs for each frame of a game\n",
        "    while not done:\n",
        "        # Pick which action to take\n",
        "        action = agent.choose_action(observation)\n",
        "\n",
        "        # Receive the results - next observation (frame), reward, done, and \n",
        "        # info (not used here)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        n_frames += 1\n",
        "        score += reward\n",
        "\n",
        "        # See if it's time to update epsilon\n",
        "        if eps > EPS_MIN and n_frames > INITIAL_EXP:\n",
        "          eps = eps - EPS_DEC\n",
        "\n",
        "        # Store the memory\n",
        "        agent.store_transition(observation, action,\n",
        "                              reward, observation_, int(done))\n",
        "        # Train on one batch\n",
        "        agent.learn() \n",
        "        # Increment observation\n",
        "        observation = observation_\n",
        "\n",
        "\n",
        "    # When game is over, add to scores\n",
        "    scores.append(score)\n",
        "\n",
        "    # If it was an evaluation game, record gif\n",
        "    if i!= 0 and i % EVALUATION_FREQUENCY == 0:\n",
        "      generate_gif(i, frames_for_gif, score)\n",
        "      frames_for_gif = []\n",
        "    \n",
        "\n",
        "    # Update average scores\n",
        "    avg_score = np.mean(scores[-50:])\n",
        "        \n",
        "    print('game: ', i,'score: ', score,\n",
        "         ' average score %.3f' % np.mean(scores[-50:]),\n",
        "        'epsilon %.2f' % eps, 'steps', n_frames)\n",
        "    \n",
        "    # Print a message. Many of these over time is an indication of learning.\n",
        "    if avg_score > best_score:\n",
        "        print('Go you! - last 50 games avg score %.2f better than best 50 games avg %.2f. ' % (\n",
        "              avg_score, best_score))\n",
        "        best_score = avg_score\n",
        "\n",
        "\n",
        "# Plot the final results    \n",
        "env.close()\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Game Number')\n",
        "plt.plot(scores)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Atari DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
